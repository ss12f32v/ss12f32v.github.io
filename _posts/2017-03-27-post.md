---
layout: post
title: "第一篇文章"
meta: "Springfield"
---

第一篇文章就拿來記錄這兩天去中研院聽演講的心得好了, 一來也是怕自己忘記, 二來是以後懂比較多時能過回來看看自己當時在哪些地方理解錯誤.

第一天的主題是 "文字探勘者的入門心法",如題目所述是在講text-mining的一些介紹和Tips, 講者是清大資工的陳宜欣老師
大抵上演講主題分為四個部份: 1.ER-model 2.Topic Mining 3.Opnion Mining 4. Contextual Text Mining 
 

第一場演講中比較有印象的是在Topic Mining中提到的LDA, LDA就是假設一個人寫文章前,就已經決定要寫幾個字,知道字的distrubution,決定要寫哪個topic,從已經知道的distribution知道要選哪個字(順帶一提, 原來LDA 也是Andrew Ng提出來的,太神啦!),在傳統LDA相關paper的精準度都很低,但在引用concept topic models時,精準度可以達到80%,甚至90%,它強調的是引用人類的智慧進去,因為通常一篇文章的重點在第一段話就能取的,不一定要讀完整篇文章;而在Opnion Mining 中比較常見的就是正負評, 其中要分析的就是TF,IDF,Co-occurence和 centrality,老師有介紹在分析中使用到的Graph construction.
中間的QA有介紹到如何在某些範例中抓做探勘,例如躁鬱症研究就跟做情緒分析有點像,還有一些在她們lab中有做到的題目(不得不說每次有人問,老師都說她們實驗室都有人正在做,有點猛), 總而言之六小時的演講也只能每個東西都點一下點一下,比較多著重在老師做這些研究時遇到的問題及處理方法,實際上要深入了解文字探勘也是需要去surver許多資料及Paper.

第二天的主題是"深入淺出深度學習",講者是台大資工的陳縕儂老師,來聽的人數比昨天至少多了100人,果然主題冠上現在最夯的機器學習人數就多了,一樣在六小時的時間中要講完現在ML的趨勢果然一點背後數學理論都不太能提到,大抵上分為三個part: 1.介紹DL,ML 2.各種NN的Structure 3.一些最近研究的趨勢

因為自己有看過一些李鴻毅老師上課的影片內容,也在這方面比較有一些些著墨,所以對老師上課說的內容比較有一些共鳴及回饋(主要是投影片內容也幾乎一模一樣),訝異的是一開始剛講到Gradient descent就有人在問Reinforcement 和 Generative model 的問題,與會者果然都是蠻厲害的,其實比較期待聽到說老師在NLP及智慧機器人的部分多做講解, 不過也是簡簡單單幾張投影片帶過,可能是時間上限制或是因為下個月又有一場演講,請待下回的概念,主要提及到的重點內容有CNN,RNN,GAN,DQN,Supervised vs Reinforcement,每一項主題都有給出現在擅長的領域,CNN之於影像處理,RNN之於文字(1-of-N Encoding),GAN就有提出一個寶可夢產生器還有漫畫人物產生器;但我自己也只有實作出運用兩層hidden layer的DNN去做出一個predict model而已,在實作出來真正有用的Application前也不能說是真正了解這些東西,只能說知道這些東西,不過好處是老師在投影片下面都有附上各個concept的reference或是paper連結,都還能再花時間去survey
在中間下課時間有去詢問老師關於我現在做的專題(Chat bot)需要注意的困難點及建議學習的東西,老師有提出兩點困難點:
1.有情緒tag的對話目前很難取得,需要花時間去找這些Training data
2.關於如何回覆需要心理學及語言學的背景知識,但如果只是固定單純回話那就不太需要
老師在後面DQN的部分有講到Task completion bot, 之後應該會在這部分去尋求真正合適專題的工具



